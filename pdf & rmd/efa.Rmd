---
title: "Exploratory factor analysis and Cronbach's alpha"
author: "Wan Nor Arifin"
date: '`r Sys.Date()`'
output:
  html_document:
    fig_caption: yes
    highlight: haddock
    number_sections: yes
    theme: null
  pdf_document:
    fig_caption: yes
    number_sections: yes
  tufte::tufte_handout:
    highlight: tango
  tufte::tufte_html:
    highlight: tango
---

```{r setup, include=FALSE}
library(tufte)
library(knitr)
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'), echo = TRUE, fig.margin=FALSE, fig.show = "asis", fig.align = "center", fig.width = 6, fig.asp = 1)
```

# Introduction
You may read my note on [Overview of validity](https://www.dropbox.com/s/qflvzp9m0p0jeh6/An%20overview%20of%20validity%20%28medstats%29.pdf?dl=0).

# Preliminaries

## Load libraries

```{r, message=FALSE}
library(foreign)
library(psych)
library(MVN)
```

## Load data set

Download data set ["Attitude_Statistics v3.sav"]("https://wnarifin.github.io/data/Attitude_Statistics v3.sav").

Read the data set as `data` and import it into a new data frame `data1` after removing **ID** variable. This will make our analysis easier because the column number = question number.
```{r}
data = read.spss("Attitude_Statistics v3.sav", use.value.labels = F, to.data.frame = T)
head(data)
data1 = data[-1]  # no ID
dim(data1)
head(data1)
```

# Exploratory factor analysis

## Preliminary steps

**Descriptive statistics:**

Check minimum-maximum values per item,
```{r}
describe(data1)
```

n (%) of response to options per item,
```{r}
response.frequencies(data1)
```

**Normality of data**

Univariate normality

1. Histograms
```{r, results='hide'}
par(mfrow = c(3,4))  # set view to 3 rows & 4 columns
apply(data1, 2, hist)
par(mfrow = c(1,1))  # set to default full view
# multi.hist(data1)  # at times, error
```

2. Shapiro Wilk's test
```{r}
apply(data1, 2, shapiro.test)
```

Multivariate normality
```{r}
mardiaTest(data1, qqplot = TRUE)
```

## Step 1

1. Check suitability of data for analysis

a. Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy
```{r}
KMO(data1)  # middling
```
b. Bartlet's test of sphericity
```{r}
cortest.bartlett(data1)  # < 0.05 = sphericity assumption met
```


2. Determine the number of factors by

a. Eigenvalues
b. Scree plot
```{r}
scree = scree(data1)  # we are only concerned with FA scree & eigenvalues
print(scree)
```
c. Parallel analysis
```{r}
parallel = fa.parallel(data1, fm = "pa", fa = "fa")
print(parallel)
```

## Step 2

1. Run EFA by fixing number of factors as decided from previous step.
2. Decide on rotation method. Choose an oblique rotation, Promax.
```{r}
fa = fa(data1, nfactors = 2, rotate = "promax", fm = "pa")
print(fa)
print(fa, cut = .3, digits = 3)
# h2 = communalities
# u2 = error variance
```

3. Assess the results:

a. Judge the quality of items. Remove poor performing items.

> Communality? Q1 < Q12 < Q2 < .25
> Pattern coeff/factor loading FL? Q1 < .3, Q12 < .4, Q2 & Q3 < .5
  
b. Check for overlap between factors.

> PA1~PA2 = .107 < .85 OK

## Step 3

1. Re-run the analysis similar to Step 2 every time an item is removed. Make judgment based on the results.
2. The analysis is finished once we have:

- satisfactory number of factors.
- satisfactory quality of items.
  
**Decisions?**  
Remove Q1? Low com & FL
```{r}
fa1 = fa(data1[-1], nfactors = 2, rotate = "promax", fm = "pa")
print(fa1, cut = .3, digits = 3)
```

Remove Q12? Low com & FL
```{r}
fa2 = fa(data1[-c(1,12)], nfactors = 2, rotate = "promax", fm = "pa")
print(fa2, cut = .3, digits = 3)
```

Remove Q2? Low com & FL
```{r}
fa3 = fa(data1[-c(1,2,12)], nfactors = 2, rotate = "promax", fm = "pa")
print(fa3, cut = .3, digits = 3)
```

Remove Q3? Low com & FL
```{r}
fa4 = fa(data1[-c(1,2,3,12)], nfactors = 2, rotate = "promax", fm = "pa")
print(fa4, cut = .3, digits = 3)
```

## Summary:
```
PA1 =~ Q4, Q5, Q6, Q7, Q11
PA2 =~ Q8, Q9, Q10
```
Name the factor.

# Cronbach's alpha

Determine the reliability for each factor separately by including the selected items only.
```{r}
names(data1)
PA1 = c("Q4","Q5","Q6","Q7","Q11")
PA2 = c("Q8","Q9","Q10")
```

**PA1**
```{r}
alpha.pa1 = alpha(data1[PA1])
print(alpha.pa1)
# r.drop = Corrected item-total correlation
# raw_alpha = Cronbach's alpha if item deleted
```

```{r}
# Squared Multiple Correlation (smc)
smc(data1[c(4,5,6,7,11)])
```

**PA2**
```{r}
alpha.pa2 = alpha(data1[PA2])
print(alpha.pa2)
smc(data1[c(8,9,10)])
```